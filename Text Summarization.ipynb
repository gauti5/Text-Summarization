{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "79c3f617-ce18-4521-8ef6-7dc776e6187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "70b4a88f-2146-489b-8455-e23cad3c51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('news_summary.csv', encoding='latin-1', engine='python') # if any error occurs then it will not goes to next lines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ba9731f0-86da-4170-a827-b3cb5c973d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d59ff015-d9f3-4b90-8ebd-66e6f1bec168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4514, 6)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "0681925b-3ea0-4fbf-ad7f-855ab7fbe97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'date', 'headlines', 'read_more', 'text', 'ctext'], dtype='object')"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "b92157c3-7f2e-49c9-947a-3e1ba66647e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author       object\n",
       "date         object\n",
       "headlines    object\n",
       "read_more    object\n",
       "text         object\n",
       "ctext        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "7baf26cd-b710-48d5-ab1a-aa4b0ab2c8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4514"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4fec6ddf-c5c8-469a-9f60-eb9d621c4cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Summary  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                                Text  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data=Data[['text', 'ctext']]\n",
    "Data.columns=['Summary', 'Text']\n",
    "Data.dropna()\n",
    "Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "fcd2d3b5-06a0-467a-ba61-a405d8b15037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4514, 2)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "9a4b827c-eb63-4b54-aada-62e8868096df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Daman and Diu administration on Wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media.The union territory?s administration was forced to retreat within 24 hours of issuing the circular that made it compulsory for its staff to celebrate Rakshabandhan at workplace.?It has been decided to celebrate the festival of Rakshabandhan on August 7. In this connection, all offices/ departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues,? the order, issued on August 1 by Gurpreet Singh, deputy secretary (personnel), had said.To ensure that no one skipped office, an attendance report was to be sent to the government the next evening.The two notifications ? one mandating the celebration of Rakshabandhan (left) and the other withdrawing the mandate (right) ? were issued by the Daman and Diu administration a day apart. The circular was withdrawn through a one-line order issued late in the evening by the UT?s department of personnel and administrative reforms.?The circular is ridiculous. There are sensitivities involved. How can the government dictate who I should tie rakhi to? We should maintain the professionalism of a workplace? an official told Hindustan Times earlier in the day. She refused to be identified.The notice was issued on Daman and Diu administrator and former Gujarat home minister Praful Kodabhai Patel?s direction, sources said.Rakshabandhan, a celebration of the bond between brothers and sisters, is one of several Hindu festivities and rituals that are no longer confined of private, family affairs but have become tools to push politic al ideologies.In 2014, the year BJP stormed to power at the Centre, Rashtriya Swayamsevak Sangh (RSS) chief Mohan Bhagwat said the festival had ?national significance? and should be celebrated widely ?to protect Hindu culture and live by the values enshrined in it?. The RSS is the ideological parent of the ruling BJP.Last year, women ministers in the Modi government went to the border areas to celebrate the festival with soldiers. A year before, all cabinet ministers were asked to go to their constituencies for the festival.'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "8cef1c69-057e-45e8-a8ca-560e320d16d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2313"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "8493050d-0265-4c17-8edf-3c011f08b64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Investigators have released pictures showing how close an Air Canada flight came to crashing onto four planes at a US airport last month. Its pilots mistakenly descended towards a taxiway, where four planes were parked, instead of the runway before aborting the landing. Investigators said the incident came within a few feet of becoming one of the worst aviation disasters.'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['Summary'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "71328d64-4131-4022-a5da-a8ecaa1a3099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data['Summary'][23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f49a8828-78f8-40eb-af1c-3165567523ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/siddanthapusandeep/Library/Python/3.12/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6f4aaf36-9cb9-4a2a-b268-b6b1f9aef8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.5.5)\n",
      "Requirement already satisfied: torch>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_lightning) (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.9.0)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_lightning) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/siddanthapusandeep/Library/Python/3.12/lib/python/site-packages (from pytorch_lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_lightning) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytorch_lightning) (0.15.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.5.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchmetrics>0.7.0->pytorch_lightning) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2e0b9680-cfe5-4d4e-8696-36d81e0c1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "473f7395-2ee9-41d0-9597-6bd19e932409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "ee880cdc-e90f-4201-a331-31cf8657eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9015d2a2-8d69-495a-badb-7a0831c7adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, source_texts, target_texts, tokenizer, source_len, target_len):\n",
    "        self.source_texts=source_texts\n",
    "        self.target_texts=target_texts\n",
    "        self.tokenizer=tokenizer\n",
    "        self.source_len=source_len\n",
    "        self.target_len=target_len\n",
    "    def __len__(self):\n",
    "        return len(self.target_texts)-1\n",
    "    def __getitem__(self, idx):\n",
    "        whitespace_handlers=lambda k:re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "        text=' '.join(str(self.source_texts[idx]).split())\n",
    "        summary=' '.join(str(self.target_texts[idx]).split())\n",
    "        \n",
    "        # handles whitespaces for text and padding will applied for max_legth(add's 0's), special characters are handled. returning tensors pytorch\n",
    "        source=self.tokenizer.batch_encode_plus([whitespace_handlers(text)],\n",
    "                                                max_length=self.source_len,\n",
    "                                                padding='max_length',\n",
    "                                                truncation=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                add_special_tokens=True,\n",
    "                                                return_tensors='pt')\n",
    "        \n",
    "        target=self.tokenizer.batch_encode_plus([whitespace_handlers(summary)],\n",
    "                                                max_length=self.target_len,\n",
    "                                                padding='max_length',\n",
    "                                                truncation=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                add_special_tokens=True,\n",
    "                                                return_tensors='pt')\n",
    "        \n",
    "        labels=target['input_ids']\n",
    "        labels[labels==0]=-100  # Replace labels==0 to -100\n",
    "        #[[2,2]] -> after squeezing [2,2]\n",
    "        return (source['input_ids'].squeeze(),\n",
    "                source['attention_mask'].squeeze(),\n",
    "                labels.squeeze(),\n",
    "                target['attention_mask'].squeeze())\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e02f8253-5fa5-439b-9dcd-2cbdec3d56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, file_path, tokenizer, batch_size, val_split_size,\n",
    "                 column_name, source_len=1024, target_len=128, corpus_size=1000):\n",
    "        super().__init__()\n",
    "        self.file_path=file_path\n",
    "        self.tokenizer=tokenizer\n",
    "        self.batch_size=batch_size\n",
    "        self.split_size=val_split_size\n",
    "        self.column_name=column_name\n",
    "        self.source_len=source_len\n",
    "        self.target_len=target_len\n",
    "        self.nrows=corpus_size\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        data=pd.read_csv(self.file_path, encoding='latin-1', nrows=self.nrows)\n",
    "        data=data[self.column_name]\n",
    "        data=data.dropna()\n",
    "        self.target_texts=data.iloc[:,0].values\n",
    "        self.source_texts=data.iloc[:, -1].values\n",
    "        \n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        X_train, X_val, y_train, y_val=train_test_split(self.source_texts, self.target_texts, test_size=self.split_size)\n",
    "        self.train_dataset=(X_train, y_train)\n",
    "        self.val_dataset=(X_val, y_val)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_data=NewsDataset(source_texts=self.train_dataset[0],\n",
    "                               target_texts=self.train_dataset[1],\n",
    "                               tokenizer=self.tokenizer,\n",
    "                               source_len=self.source_len,\n",
    "                               target_len=self.target_len)\n",
    "        return DataLoader(train_data, self.batch_size, num_workers=0, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_data=NewsDataset(source_texts=self.val_dataset[0],\n",
    "                             target_texts=self.val_dataset[1],\n",
    "                             tokenizer=self.tokenizer,\n",
    "                             source_len=self.source_len,\n",
    "                             target_len=self.target_len)\n",
    "        return DataLoader(val_data, self.batch_size, num_workers=0, pin_memory=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d5800ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fintuner(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.training_losses = []   # store losses per batch\n",
    "        self.validation_losses = [] # store val losses per batch\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_attention_mask=None, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs.loss \n",
    "    \n",
    "    def _step(self, batch):\n",
    "        source_input_ids, source_attention_mask, target_input_ids, target_attention_mask = batch\n",
    "        loss = self(\n",
    "            input_ids=source_input_ids,\n",
    "            attention_mask=source_attention_mask,\n",
    "            decoder_attention_mask=target_attention_mask,\n",
    "            labels=target_input_ids\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.training_losses.append(loss.detach())   # save batch loss\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.validation_losses.append(loss.detach()) # save batch val loss\n",
    "        return {\"val_loss\": loss}\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.training_losses:  # aggregate batch losses\n",
    "            avg_loss = torch.stack(self.training_losses).mean()\n",
    "            self.log('train_loss', avg_loss, prog_bar=True, logger=True)\n",
    "            self.training_losses.clear()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.validation_losses:\n",
    "            avg_val_loss = torch.stack(self.validation_losses).mean()\n",
    "            self.log('val_loss', avg_val_loss, prog_bar=True, logger=True)\n",
    "            self.validation_losses.clear()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=optimizer, mode='min', factor=0.1, patience=3\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "8fe1997e-9d6a-4dec-b2ef-a33ee8bcddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer=AutoTokenizer.from_pretrained('t5-small')\n",
    "\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e7326b27-96aa-4314-bb39-c2ac553134e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=NewsDataLoader(tokenizer=tokenizer, file_path='news_summary.csv', batch_size=4,\n",
    "                          val_split_size=0.3, column_name=['text', 'ctext'])\n",
    "\n",
    "dataloader.prepare_data()\n",
    "dataloader.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5c3723c2-c2da-4f0e-8f1f-24e2332d4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=fintuner(model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "f68a3c64-97bb-4173-b279-aeb9d4f9fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9eaeb836-672b-46ab-afbc-e0584873617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callbacks=ModelCheckpoint(\n",
    "    dirpath='checkpoints',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "logger=TensorBoardLogger('lightning_logs', name='summary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3ccd231b-57bd-4e1b-bb76-4232f17110ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "early_stop_callback=EarlyStopping(monitor='val_loss', patience=5, verbose=False, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "af65464c-ce05-4c0e-9a7f-03473b47acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(check_val_every_n_epoch=1, max_epochs=1, accelerator='gpu', callbacks=[early_stop_callback, checkpoint_callbacks],\n",
    "                   logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8e033115-8bd3-4efc-8217-54ecd09abe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                       | Params | Mode\n",
      "------------------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M | eval\n",
      "------------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "277       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 173/173 [11:55<00:00,  0.24it/s, v_num=4, val_loss=1.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 173: 'val_loss' reached 1.80413 (best 1.80413), saving model to '/Users/siddanthapusandeep/Data Science/Projects/Text-Summarization/checkpoints/best-checkpoint.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 173/173 [12:01<00:00,  0.24it/s, v_num=4, val_loss=1.800]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if __name__ == \"__main__\":   # ✅ add this guard\n",
    "    trainer.fit(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
